---
title: 12월 세미나 모음
author: Chaewon Kim
date: 2025-12-24
category: Jekyll
layout: post
---


# 2025-12-24 (화) 

특이사항 : 첫 세미나 청취 

모르는 키워드 위주로 정리 시작 

어제 진행했던 세미나 간단하게 복기하기 

배경 키워드 

**Vector Search**
대규모 컬렉션에서 일반적으로 벡터로 표현되는 유사한 항목이나 데이터 포인트를 찾는 데 사용되는 검색 기술

벡터는 요소 간의 의미적 관계를 포착하여 머신 러닝 모델과 인공 지능 애플리케이션의 효과적인 처리를 가능하게 함

비유가 이해하기 쉬워서 가져옴 


전통적인 키워드 검색과 벡터 검색 간의 차이점을 설명하기 위해 예를 살펴보겠습니다. 최고의 피자 레스토랑에 대한 정보를 찾고 있고 전통적인 키워드 검색 엔진에서 "최고의 피자 레스토랑"을 검색한다고 가정합시다. 키워드 검색은 "최고의", "피자" 및 "레스토랑"이라는 정확한 단어가 포함된 페이지를 찾고 "최고의 피자 레스토랑" 또는 "내 주변 피자 레스토랑"과 같은 결과만 반환합니다. 전통적인 키워드 검색은 검색 이면의 맥락이나 의도를 이해하기보다는 **키워드를 일치시키는 데 중점**을 둡니다.

반면, 시맨틱 벡터 검색의 검색 엔진은 **쿼리 이면의 의도**를 이해합니다. 

시맨틱(semantic)은 정의상 언어의 의미와 관련이 있으며 시맨틱 검색은 쿼리의 의미와 맥락을 이해합니다. 그래서 콘텐츠에 "최고의 피자 레스토랑"이라는 정확한 단어가 사용되지 않았어도 최고 평점 또는 추천이 많은 피자 가게에 대해 이야기하는 콘텐츠를 찾습니다. 맥락과 관련성이 더 큰 결과를 제공하며 다양한 위치에 있는 고품질 피자 가게에 대해 설명하는 기사나 가이드를 포함할 수 있습니다.

![image](../assets/Semina/2025-12-23/vector_search.png)


출처 : https://www.ibm.com/kr-ko/think/topics/vector-search 


**ANN (근사 최근접 이웃)**
ANN 알고리즘은 정확한 일치를 찾는 것이 아니라 유클리드 거리 또는 코사인 유사성과 같은 일부 거리 메트릭을 기반으로 지정된 쿼리에 거의 가장 가까운 벡터를 효율적으로 검색합니다. 
이러한 알고리즘은 일정 수준의 근사치를 허용하므로 전체 코퍼스에서 임베딩 유사성을 계산할 필요 없이 최근접 이웃 검색의 계산 비용을 크게 줄일 수 있습니다.


논문 키워드 (솔직히 잘 몰라서 제미나이 도움 받음 )

dataset이 billion scale인 상황에서 searching 시 Query성능은 유지하면서 latency를 낮추는 것을 목표 


**DiskANN(Graph base) vs SPANN(Cluster base)**

    두 알고리즘 모두 메모리 제약을 극복하고 디스크에서 Billion-scale 데이터를 검색하기 위해 설계되었습니다.

    DiskANN (Graph-based): * 핵심: Vamana라는 강력한 그래프 인덱스를 사용합니다.

        특징: 데이터 간의 연결 관계(Edge)를 따라가며 검색합니다. SSD의 병렬 읽기 성능을 극대화하여, 적은 메모리로도 메모리 기반 알고리즘에 근접하는 높은 재현율(Recall)과 낮은 지연시간(Latency)을 보여줍니다.

    SPANN (Cluster-based): * 핵심: 데이터를 여러 개의 클러스터로 나누고, 각 클러스터의 중심점(Centroid)만 메모리에 올립니다.

        특징: 쿼리가 들어오면 가까운 클러스터들을 찾고, 해당 클러스터가 저장된 디스크 블록만 읽습니다. 그래프 방식보다 구조가 단순하며 특정 조건에서 더 빠른 검색 속도를 보여주기도 합니다.



Billion-scale 검색 시스템의 병목 현상을 해결하기 위한 프로토콜

**LIAR Protocol(Low-latency In-memory Adaptive Rescoring)**

    디스크에서 읽어온 후보군들을 메모리에서 다시 계산(Rescoring)할 때, 성능에 맞춰 적응적으로 처리하는 방식입니다. 불필요한 계산을 줄여 쿼리 응답 속도를 최적화합니다.    

**NAP(Neighbor-Aware Prefetching)**

    그래프 검색 시 다음에 방문할 가능성이 높은 이웃 노드들을 미리 예측해서 디스크에서 메모리로 가져오는(Prefetching) 기술입니다. 디스크 I/O 대기 시간을 줄여 Latency를 획기적으로 낮춥니다.

**local rebuilder**


데이터가 실시간으로 추가/수정될 때 인덱스를 어떻게 유지할 것인가에 대한 내용입니다.

**Out of place update vs In place update**

    In-place: 기존 데이터가 저장된 위치에 그대로 덮어쓰는 방식입니다. 공간 효율은 좋지만, 수정 중에 검색 성능이 떨어지거나 데이터가 깨질 위험이 있습니다.

    Out-of-place: 기존 데이터는 두고 새로운 위치에 데이터를 쓴 뒤 포인터를 바꿉니다. 안정적이고 Append-only 구조에 적합합니다.

**append only**

    데이터를 수정하거나 삭제하지 않고 무조건 뒤에 덧붙이는 방식입니다. 
    쓰기 성능이 매우 빠르고 로그 구조 저장소(LSM-tree 등)에서 자주 쓰이며, 검색 시에는 최신 버전만 참조합니다.

**NNS(Nearest Neighbor Search)**

    고차원 공간에서 특정 쿼리 벡터와 가장 유사한(거리가 가까운) 데이터를 찾는 근본적인 문제

**요약 : 데이터 변경 시 벡터값 생성 -> 위치 이동 -> 주변 이웃/클러스터 재설정 과정이 필요하며, 이 작업을 시스템 성능 저하 없이 실시간으로 처리하는 것이 이 분야의 핵심 기술**



키워드 정리

    파일 시스템은 갑작스러운 전원 차단 시 데이터가 깨지는 것을 막기 위해 여러 장치를 둡니다.

**Jounaling**

    데이터를 실제로 저장하기 전에, "어떤 작업을 할 것인지"에 대한 로그(Journal)를 먼저 남기는 방식입니다. 사고가 나도 이 로그를 보고 복구할 수 있습니다.

**transcaction**

    여러 개의 작업(파일 생성, 크기 수정, 시간 업데이트 등)을 하나의 묶음으로 처리하는 단위입니다. "모두 성공하거나, 아니면 아예 하나도 안 한 상태로 되돌리거나(All or Nothing)"를 보장합니다.

**transcation lock up**

    저널링 과정에서 너무 많은 트랜잭션이 한꺼번에 몰리거나, 특정 작업이 길어지면서 전체 시스템이 일시적으로 멈추는(Freezing) 현상

**CXL memory**

    CPU와 메모리, 저장장치를 더 빠르게 연결하는 차세대 인터페이스입니다.

**built in cache**

    파일 시스템 내부에 구현된 캐시 영역입니다. 자주 사용하는 메타데이터나 데이터를 메모리에 상주시켜 디스크 I/O를 최소화합니다.

**DJFS(Decoupled Journaling File System)**

    전통적인 파일 시스템은 데이터와 저널을 같은 장치에 쓰는데, DJFS는 이 **저널링 과정을 분리(Decoupled)**하여 최적화한 파일 시스템입니다.

    보통 비휘발성 메모리(NVM)나 CXL 메모리에 저널만 따로 저장해 성능을 극대화하는 논문 등에서 자주 등장하는 개념입니다.


**commit**

    트랜잭션 작업이 성공적으로 완료되어 변경 사항을 파일 시스템에 영구적으로 반영하는 최종 단계입니다.

**shadow copy**

    원본 데이터를 직접 수정하지 않고, 복사본을 만들어 수정하는 방식입니다. 작업 중에 문제가 생겨도 원본은 안전하며, 스냅샷(Snapshot) 기능을 구현할 때 핵심입니다.

**CMM-H(CXL Memory Module - Hybrid)**

    기존의 저장 장치는 'DRAM(매우 빠름, 용량 작음)'과 'SSD(느림, 용량 큼)'로 완전히 분리되어 있었습니다. CMM-H는 이 둘을 하나의 CXL 장치 안에 합친 하이브리드 모델입니다.

    구조: 장치 내부에 DRAM 캐시 + **NAND 플래시(SSD)**가 같이 들어있습니다.

    특징: Near-DRAM 성능: 자주 쓰는 데이터는 내부 DRAM 캐시에서 처리해 DRAM급 속도를 냅니다.

        NAND급 용량: 실제 데이터는 NAND에 저장하므로 테라바이트(TB) 단위의 대용량을 제공합니다.

        Persistence (비휘발성): 전원이 꺼져도 배터리 등을 이용해 DRAM의 데이터를 NAND로 옮겨 보존합니다.


# 2025-12-30 (화) 

처음 들어보는 키워드 위주로 정리하기 


**Phony Buffer**
리눅스 커널이 GPU 메모리로 직접 I/O를 수행할 때, 이를 시스템 메모리처럼 인식하게 만들기 위해 생성하는 가짜 구조체(struct page)로, 성능 저하의 원인이 됨.
    왜 생겨났는가? (Legacy GDS의 한계) 리눅스 커널은 파일 입출력(I/O)을 수행할 때, 데이터가 저장될 대상이 커널이 관리하는 정식 메모리 구조체인 struct page를 가지고 있기를 기대합니다. 하지만 GPU 메모리는 커널 입장에서 직접 관리하지 않는 외부 장치 메모리이기 때문에, 이 struct page가 존재하지 않습니다.

    꼼수: 가짜(Phony) 버퍼의 생성 기존 GDS(GPU Direct Storage)는 이 규칙을 우회하기 위해 시스템 메모리(RAM)에 가짜 버퍼를 만듭니다. 커널에게는 "나 시스템 메모리에 데이터 쓸 거야"라고 거짓으로 struct page를 보여주고, 실제 하드웨어(DMA) 레벨에서는 데이터를 GPU로 바로 보냅니다. 이때 커널을 속이기 위해 만든, 실제로는 쓰이지 않는 시스템 메모리 객체를 Phony Buffer라고 부릅니다.

    문제점: 성능 저하 이 방식은 작동은 하지만 비효율적입니다. 매 I/O 요청마다 가짜 버퍼를 할당하고 해제하는 과정이 반복되며, 메타데이터를 초기화하는 CPU 비용이 낭비됩니다. 결과적으로 빠른 SSD를 써도 이 서류 작업 때문에 Latency(지연 시간)가 늘어납니다.

    Phoenix의 해결책 (Zone Device) Phoenix는 ZONE_DEVICE 기능을 사용해 GPU 메모리를 커널의 정식 메모리 영역으로 등록합니다. 이렇게 하면 가짜 버퍼를 만들 필요 없이 커널이 GPU 메모리를 직접 관리할 수 있게 되어 불필요한 오버헤드가 사라지고 속도가 빨라집니다.

**GDS**
CPU와 시스템 메모리를 거치지 않고, NVMe 스토리지에서 GPU 메모리로 데이터를 직접 전송하여 대역폭을 높이는 기술.


**Phoenix**
기존 GDS의 오버헤드(Phony Buffer)를 제거하고 I/O 경로를 최적화하여 지연 시간을 줄인 새로운 I/O 스택.


**Zone device**
GPU 메모리와 같은 장치 메모리를 CPU의 페이지 테이블에 직접 매핑하여, 커널이 이를 일반 메모리처럼 관리할 수 있게 해주는 리눅스 기능.


**POSIX API**
read, write 같은 표준 파일 입출력 인터페이스로, Phoenix는 이를 준수하여 기존 코드 수정 없이 고성능 I/O를 사용할 수 있게 함.


**KV cache**
대규모 언어 모델(LLM) 추론 시 중복 연산을 방지하기 위해 저장하는 이전 토큰들의 연산 결과값.


**IO stack latency**
운영체제 커널, 파일 시스템, 드라이버 등 소프트웨어 계층을 데이터가 통과하는 데 걸리는 지연 시간.


**cheakpoint loading**
학습 중이거나 완료된 모델의 파라미터(가중치) 파일을 디스크에서 GPU 메모리로 불러오는 과정입니다. LLM은 용량이 매우 크기 때문에(수백 GB~TB), 이 로딩 속도가 서비스 시작 시간(Cold Start)을 좌우합니다.

---

**char device file**
데이터를 블록(Block) 단위가 아닌, 문자(Byte) 단위의 스트림으로 순차적으로 입출력하는 장치 파일입니다. (예: 키보드, 마우스, 그리고 GPU) 리눅스에서 /dev/nvidia0 같은 GPU 장치 파일은 Char device로 취급되며, 보통 시스템 버퍼를 거치지 않고 직접 통신합니다.


**mmap**
파일의 내용을 프로세스의 가상 메모리 주소 공간에 직접 매핑하는 시스템 콜입니다. read, write 처럼 데이터를 복사하는 오버헤드 없이, 파일을 마치 메모리 배열처럼 다룰 수 있어 대용량 모델 로딩 시 필수적입니다.


**QLLM**
모델의 가중치(Weight) 표현 정밀도를 줄여(예: 16bit 실수 → 4bit 정수) 모델의 크기와 연산량을 압축한 모델입니다. 약간의 정확도 손실을 감수하고, 메모리 사용량을 대폭 줄여서 더 작은 GPU에서도 거대 모델을 구동할 수 있게 합니다.


**self attension**
트랜스포머의 핵심 원리로, 문장 내의 각 단어가 문장 내의 다른 모든 단어들과 어떤 연관성(가중치)을 가지는지 계산하는 과정입니다. "그것(It)"이라는 단어가 나왔을 때, 문맥상 이것이 강아지를 가리키는지 자동차를 가리키는지 파악하기 위해 문장 전체를 훑어보는 집중력과 같습니다.


**Priority schedulor**
모든 작업을 동등하게 처리하지 않고, 중요도(Priority)가 높은 작업에 자원을 먼저 할당하는 방식입니다. (예: 실시간 사용자 요청을 백그라운드 학습 작업보다 먼저 처리)


**FCFS**
가장 먼저 도착한 요청을 먼저 처리하는 선입선출(FIFO) 방식의 가장 단순한 스케줄링입니다. 긴 작업이 앞을 막으면 뒤의 작업들이 모두 지연되는 단점이 있습니다.


**SLO**
서비스 수준 목표, 즉 서비스 제공자가 보장해야 하는 기술적 목표 성능 수치입니다. "99%의 요청에 대해 200ms 이내에 응답한다"와 같은 구체적인 기준을 의미합니다.


**Mixture of Expert Model(MoE)**
하나의 거대한 모델을 쓰는 대신, 특정 분야에 특화된 여러 개의 작은 서브 네트워크(Experts)를 구성하고, 입력 데이터에 따라 필요한 전문가만 골라서 연산하는 아키텍처입니다. 모델의 전체 파라미터 개수는 엄청나게 키우면서도, 실제 추론 시에는 일부만 활성화하므로 연산 비용을 획기적으로 줄일 수 있습니다.


**Batch 엔진**
들어오는 개별 요청들을 하나씩 처리하지 않고, 여러 개를 묶어(Batch) 한 번에 GPU 연산을 수행하도록 관리하는 시스템입니다. GPU는 대량의 행렬 연산을 병렬 처리할 때 효율이 극대화되므로, 처리량(Throughput)을 높이기 위해 필수입니다.


**Transformer Model**
현대 NLP(자연어 처리)와 LLM의 근간이 되는 딥러닝 아키텍처입니다. 순차적으로 데이터를 처리하던 기존 RNN과 달리, 문장 전체를 한 번에 병렬로 처리하며 Attention 메커니즘을 통해 단어 간의 관계를 파악합니다.
